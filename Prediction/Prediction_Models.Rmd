---
title: "Predictive Models"
author: "Elena Marochkina"
date: "`r Sys.Date()`"
output: 
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  echo = FALSE,
  message = FALSE
)

```

```{r libraries}
library(dplyr)
library(caret)
library(rpart)
library(rpart.plot)
library(pROC)
library(randomForest)
library(performanceEstimation)  
library(xgboost)
library(ROSE)
library(MLmetrics)
    
```

```{r Metrics function}
# Function to print metrics
print_metrics <- function(model, test_data, test_probabilities, test_predictions) {
  # Confusion Matrix
  conf_matrix <- confusionMatrix(test_predictions, test_data$SURVIVAL_FLAG)
  cat("Confusion Matrix:\n")
  print(conf_matrix)
  
  # Plot ROC curve
  roc_curve <- roc(test_data$SURVIVAL_FLAG, test_probabilities) 
  plot(
    roc_curve,
    col = "darkorange",
    lwd = 2,
    main = "ROC Curve",
    print.auc = TRUE
  )
  
  # Metrics
  accuracy <- Accuracy(test_data$SURVIVAL_FLAG, test_predictions)
  sensitivity <- Recall(test_data$SURVIVAL_FLAG, test_predictions)
  specificity <- Specificity(test_data$SURVIVAL_FLAG, test_predictions)
  precision <- Precision(test_data$SURVIVAL_FLAG, test_predictions)

  # Calculate F1-score
  f1_score <- 2 * (precision * sensitivity) / (precision + sensitivity)
  AUC <- auc(roc_curve)

  cat("Accuracy:", accuracy, "\n")
  cat("Sensitivity (Recall):", sensitivity, "\n")
  cat("Specificity:", specificity, "\n")
  cat("Precision:", precision, "\n")
  cat("F1-score:", f1_score, "\n")
  cat("AUC:", AUC, "\n")
}
```

# 1. Outcomes Data

## 1.1. Read Data

```{r read data}
death_status <- read.csv("../data/prepared_to_prediction/Death_status.csv", stringsAsFactors = TRUE)

# Ensure SURVIVAL_FLAG is a factor for classification
death_status <- death_status %>%
  mutate(SURVIVAL_FLAG = as.factor(SURVIVAL_FLAG)) %>%
  select(-SUBJECT_ID_COMPOSE, -SUBJECT_ID, -BG_LAST_VENTILATOR)
```

```{r missing values}
death_status <- death_status %>%
  mutate(across(
    where(is.numeric), 
    ~ round(ifelse(is.na(.), mean(., na.rm = TRUE), .), 2)
  ))
```

# 2. Death Status
## 2.1. Desicion Tree Model

```{r Decision Tree Model unbalanced}
# Set seed for reproducibility
set.seed(12)

# Split the data into training (60%), validation (20%), and testing (20%)
train_val_index <- createDataPartition(death_status$SURVIVAL_FLAG, p = 0.8, list = FALSE)
train_val_data <- death_status[train_val_index, ]
test_data <- death_status[-train_val_index, ]

train_index <- createDataPartition(train_val_data$SURVIVAL_FLAG, p = 0.75, list = FALSE)
train_data <- train_val_data[train_index, ]
validation_data <- train_val_data[-train_index, ]

# Train the Decision Tree with tuned parameters
decision_tree_tuned_unbalanced <- rpart(
  SURVIVAL_FLAG ~ .,
  data = train_data,
  method = "class",
  control = rpart.control(cp = 0.01, minsplit = 20, maxdepth = 5)
)

# Visualize the Decision Tree
rpart.plot(decision_tree_tuned_unbalanced, type = 2, extra = 104, tweak = 1.2, main = "Tuned Decision Tree")

# Make predictions on the validation set
probabilities_unbalanced <- predict(decision_tree_tuned_unbalanced, validation_data, type = "prob")
predictions_unbalanced <- predict(decision_tree_tuned_unbalanced, validation_data, type = "class")

# Call the function with predictions and probabilities
print_metrics(decision_tree_tuned_unbalanced, validation_data, probabilities_unbalanced[, 2], predictions_unbalanced)

```

```{r Decision Tree Model undersampling}

# Apply undersampling to balance the dataset
desired_majority_class_UND <- sum(train_data$SURVIVAL_FLAG == 1) # Match majority size to minority size
total_size_UND <- 2 * desired_majority_class_UND

# Adjust the proportion to balance the classes without reducing too much
train_data_balanced_UND <- ovun.sample(
  SURVIVAL_FLAG ~ ., 
  data = train_data, 
  method = "under", 
  N = total_size_UND, 
  seed = 123
)$data

# Check class distribution after SMOTE
cat("Class distribution after balance:\n")
print(table(train_data_balanced_UND$SURVIVAL_FLAG))

# Train the Decision Tree with tuned parameters
decision_tree_UND <- rpart(
  SURVIVAL_FLAG ~ .,
  data = train_data_balanced_UND,
  method = "class",
  control = rpart.control(cp = 0.01, minsplit = 20, maxdepth = 5)
)

# Visualize the Decision Tree
rpart.plot(decision_tree_UND, type = 2, extra = 104, tweak = 1.2, main = "Tuned Decision Tree (Undersampling balanced)")

# Make predictions on the validation set
probabilities_UND <- predict(decision_tree_UND, validation_data, type = "prob")
predictions_UND <- predict(decision_tree_UND, validation_data, type = "class")

# Call the function with predictions and probabilities
print_metrics(decision_tree_UND, validation_data, probabilities_UND[, 2], predictions_UND)

```


## 2.2. Random Forest Model
```{r Random Forest Model simple}
# ------------------------------------------------------------------------
# HYPERPARAMETER TUNING
# ------------------------------------------------------------------------

# Set up the grid of hyperparameters for tuning
tune_grid <- expand.grid(
  mtry = c(10, 12, 15, 20, 30),   
  ntree = c(100, 200, 300, 400, 500), # Number of trees
  maxnodes = c(10, 20, 30, 40, 50)  # Maximum nodes
)

# Tune the model using validation set
best_model <- NULL
best_auc <- 0

for (i in 1:nrow(tune_grid)) {
  cat("Testing configuration:", paste(tune_grid[i, ], collapse = ", "), "\n")
  model <- randomForest(
    SURVIVAL_FLAG ~ ., 
    data = train_data, 
    ntree = tune_grid[i, "ntree"], 
    mtry = tune_grid[i, "mtry"], 
    maxnodes = tune_grid[i, "maxnodes"],
    importance = TRUE
  )
  
# Evaluate on validation set
  validation_probabilities <- predict(model, validation_data, type = "prob")[, 2]
  roc_curve <- roc(validation_data$SURVIVAL_FLAG, validation_probabilities)
  auc <- auc(roc_curve)
  
  cat("AUC for current configuration:", auc, "\n")
  
  if (auc > best_auc) {
    best_model <- model
    best_auc <- auc
  }
}

cat("Best AUC:", best_auc, "\n")

# Metrics
probabilities <- predict(best_model, validation_data, type = "prob")[, 2]
predictions <- predict(best_model, validation_data, type = "class")

# Call the function with predictions and probabilities
print_metrics(best_model, validation_data, probabilities, predictions)

```

```{r Random Forest Model balanced}
# Apply undersampling to balance the training dataset
desired_majority_class_RF_UND <- sum(train_data$SURVIVAL_FLAG == 1) # Match majority size to minority size
total_size <- 2 * desired_majority_class_RF_UND

train_data_balanced_RF_UND <- ovun.sample(
  SURVIVAL_FLAG ~ ., 
  data = train_data, 
  method = "under", 
  N = total_size, 
  seed = 123
)$data

# Check the class distribution after balancing
cat("Balanced Training Class Distribution:\n")
print(table(train_data_balanced_RF_UND$SURVIVAL_FLAG))

# ------------------------------------------------------------------------
# HYPERPARAMETER TUNING
# ------------------------------------------------------------------------

# Set up the grid of hyperparameters for tuning
tune_grid <- expand.grid(
  mtry = c(10, 12, 15, 20, 30),   
  ntree = c(100, 200, 300, 400, 500), # Number of trees
  maxnodes = c(10, 20, 30, 40, 50)  # Maximum nodes
)

# Tune the model using validation set
best_model <- NULL
best_auc <- 0

for (i in 1:nrow(tune_grid)) {
  cat("Testing configuration:", paste(tune_grid[i, ], collapse = ", "), "\n")
  model <- randomForest(
    SURVIVAL_FLAG ~ ., 
    data = train_data_balanced_RF_UND, 
    ntree = tune_grid[i, "ntree"], 
    mtry = tune_grid[i, "mtry"], 
    maxnodes = tune_grid[i, "maxnodes"],
    importance = TRUE
  )
  
  # Evaluate on validation set
  validation_probabilities <- predict(model, validation_data, type = "prob")[, 2]
  roc_curve <- roc(validation_data$SURVIVAL_FLAG, validation_probabilities)
  auc <- auc(roc_curve)
  
  cat("AUC for current configuration:", auc, "\n")
  
  if (auc > best_auc) {
    best_model <- model
    best_auc <- auc
  }
}

cat("Best AUC:", best_auc, "\n")

# Metrics
probabilities <- predict(best_model, validation_data, type = "prob")[, 2]
predictions <- predict(best_model, validation_data, type = "class")

# Call the function with predictions and probabilities
print_metrics(best_model, validation_data, probabilities, predictions)
```

```{r Random Forest Model weighted}
# Compute class weights based on training data
class_counts <- table(train_data$SURVIVAL_FLAG)
total_samples <- sum(class_counts)
class_weights <- total_samples / (length(class_counts) * class_counts)
print("Class Weights:")
print(class_weights)

# ------------------------------------------------------------------------
# HYPERPARAMETER TUNING
# ------------------------------------------------------------------------

# Set up the grid of hyperparameters for tuning
tune_grid <- expand.grid(
  mtry = c(10, 12, 15, 20, 30),   
  ntree = c(100, 200, 300, 400, 500), # Number of trees
  maxnodes = c(10, 20, 30, 40, 50)  # Maximum nodes
)

# Tune the model using validation set
best_model <- NULL
best_auc <- 0

for (i in 1:nrow(tune_grid)) {
  cat("Testing configuration:", paste(tune_grid[i, ], collapse = ", "), "\n")
  model <- randomForest(
    SURVIVAL_FLAG ~ ., 
    data = train_data, 
    ntree = tune_grid[i, "ntree"], 
    mtry = tune_grid[i, "mtry"], 
    maxnodes = tune_grid[i, "maxnodes"],
    importance = TRUE,
    classwt = class_weights
  )
  
  # Evaluate on validation set
  validation_probabilities <- predict(model, validation_data, type = "prob")[, 2]
  roc_curve <- roc(validation_data$SURVIVAL_FLAG, validation_probabilities)
  auc <- auc(roc_curve)
  
  cat("AUC for current configuration:", auc, "\n")
  
  if (auc > best_auc) {
    best_model <- model
    best_auc <- auc
  }
}

cat("Best AUC:", best_auc, "\n")

# Metrics
probabilities <- predict(best_model, validation_data, type = "prob")[, 2]
predictions <- predict(best_model, validation_data, type = "class")

# Call the function with predictions and probabilities
print_metrics(best_model, validation_data, probabilities, predictions)
```

## 2.3. XGBoost Model
```{r XGBoost Model}
# Prepare predictor columns
predictor_columns <- setdiff(names(train_data), "SURVIVAL_FLAG")

# Ensure the data is properly formatted for XGBoost
dummies <- dummyVars(~ ., data = train_data[, predictor_columns])
train_matrix <- predict(dummies, newdata = train_data[, predictor_columns])
validation_matrix <- predict(dummies, newdata = validation_data[, predictor_columns])
test_matrix <- predict(dummies, newdata = test_data[, predictor_columns])

# Convert to xgb.DMatrix
train_xgb <- xgb.DMatrix(data = as.matrix(train_matrix), label = as.numeric(train_data$SURVIVAL_FLAG) - 1)
val_xgb <- xgb.DMatrix(data = as.matrix(validation_matrix), label = as.numeric(validation_data$SURVIVAL_FLAG) - 1)
test_xgb <- xgb.DMatrix(data = as.matrix(test_matrix), label = as.numeric(test_data$SURVIVAL_FLAG) - 1)

# ------------------------------------------------------------------------
# HYPERPARAMETER TUNING
# ------------------------------------------------------------------------

# Define a grid of hyperparameters
param_grid <- expand.grid(
  max_depth = c(4, 6, 8),
  eta = c(0.01, 0.1, 0.3),
  gamma = c(0, 1, 5),
  subsample = c(0.7, 0.8, 1.0),
  colsample_bytree = c(0.6, 0.8, 1.0)
)

# Initialize a results data frame
tuning_results <- data.frame()

for (i in 1:nrow(param_grid)) {
  # Extract parameters for this iteration
  params <- param_grid[i, ]
  
  xgb_params <- list(
    objective = "binary:logistic",
    eval_metric = "auc",
    max_depth = params$max_depth,
    eta = params$eta,
    gamma = params$gamma,
    subsample = params$subsample,
    colsample_bytree = params$colsample_bytree
  )
  
  # Train the model with validation monitoring
  set.seed(123)
  model <- xgb.train(
    params = xgb_params,
    data = train_xgb,
    nrounds = 100,
    watchlist = list(train = train_xgb, validation = val_xgb),
    early_stopping_rounds = 10,
    verbose = 0
  )
  
  # Evaluate performance on validation data
  val_predictions <- predict(model, val_xgb)
  auc <- auc(roc(validation_data$SURVIVAL_FLAG, val_predictions))
  
  # Store the results
  tuning_results <- rbind(
    tuning_results,
    cbind(params, AUC = auc)
  )
}

# Find the best parameters
best_params <- tuning_results[which.max(tuning_results$AUC), ]
cat("Best Parameters:\n")
print(best_params)

# ------------------------------------------------------------------------
# TRAIN FINAL MODEL WITH BEST PARAMETERS
# ------------------------------------------------------------------------


# Train the model on the training set
final_xgb_params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  max_depth = best_params$max_depth,
  eta = best_params$eta,
  gamma = best_params$gamma,
  subsample = best_params$subsample,
  colsample_bytree = best_params$colsample_bytree
)

final_xgb_model <- xgb.train(
  params = final_xgb_params,
  data = train_xgb,     # Only training data
  nrounds = 100,
  watchlist = list(train = train_xgb, validation = val_xgb),
  early_stopping_rounds = 10,
  verbose = 1
)

# Evaluate the model on the validation set
val_predictions <- predict(final_xgb_model, val_xgb)
val_class <- ifelse(val_predictions > 0.5, 1, 0)

# Ensure validation data target is a factor
validation_data$SURVIVAL_FLAG <- as.factor(validation_data$SURVIVAL_FLAG)
val_class <- factor(val_class, levels = levels(validation_data$SURVIVAL_FLAG))

# Evaluate performance on validation set
print_metrics(final_xgb_model, validation_data, val_predictions, val_class)

```

```{r XGBoost Model balanced}
# Apply undersampling to balance the training dataset
library(ROSE)
train_data_balanced <- ovun.sample(
  SURVIVAL_FLAG ~ ., 
  data = train_data, 
  method = "under",
  seed = 123
)$data

cat("Balanced Class Distribution:\n")
print(table(train_data_balanced$SURVIVAL_FLAG))

# ------------------------------------------------------------------------
# FORMAT DATA FOR XGBOOST
# ------------------------------------------------------------------------

# Prepare predictor columns
predictor_columns <- setdiff(names(train_data_balanced), "SURVIVAL_FLAG")

# Create dummy variables for categorical predictors
dummies <- dummyVars(~ ., data = train_data_balanced[, predictor_columns])
train_matrix <- predict(dummies, newdata = train_data_balanced[, predictor_columns])
validation_matrix <- predict(dummies, newdata = validation_data[, predictor_columns])
test_matrix <- predict(dummies, newdata = test_data[, predictor_columns])

# Convert to xgb.DMatrix
train_xgb <- xgb.DMatrix(data = as.matrix(train_matrix), label = as.numeric(train_data_balanced$SURVIVAL_FLAG) - 1)
val_xgb <- xgb.DMatrix(data = as.matrix(validation_matrix), label = as.numeric(validation_data$SURVIVAL_FLAG) - 1)
test_xgb <- xgb.DMatrix(data = as.matrix(test_matrix), label = as.numeric(test_data$SURVIVAL_FLAG) - 1)

# ------------------------------------------------------------------------
# HYPERPARAMETER TUNING
# ------------------------------------------------------------------------

# Define a grid of hyperparameters
param_grid <- expand.grid(
  max_depth = c(4, 6, 8),
  eta = c(0.01, 0.1, 0.3),
  gamma = c(0, 1, 5),
  subsample = c(0.7, 0.8, 1.0),
  colsample_bytree = c(0.6, 0.8, 1.0)
)

# Initialize a results data frame
tuning_results <- data.frame()

for (i in 1:nrow(param_grid)) {
  params <- param_grid[i, ]
  
  xgb_params <- list(
    objective = "binary:logistic",
    eval_metric = "auc",
    max_depth = params$max_depth,
    eta = params$eta,
    gamma = params$gamma,
    subsample = params$subsample,
    colsample_bytree = params$colsample_bytree
  )
  
  # Train the model with validation monitoring
  model <- xgb.train(
    params = xgb_params,
    data = train_xgb,
    nrounds = 100,
    watchlist = list(train = train_xgb, validation = val_xgb),
    early_stopping_rounds = 10,
    verbose = 0
  )
  
  # Evaluate performance on validation data
  val_predictions <- predict(model, val_xgb)
  auc_score <- auc(roc(validation_data$SURVIVAL_FLAG, val_predictions))
  
  # Store the results
  tuning_results <- rbind(
    tuning_results,
    cbind(params, AUC = auc_score)
  )
}

# Find the best parameters
best_params <- tuning_results[which.max(tuning_results$AUC), ]
cat("Best Parameters:\n")
print(best_params)

# ------------------------------------------------------------------------
# TRAIN FINAL MODEL WITH BEST PARAMETERS
# ------------------------------------------------------------------------

final_xgb_params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  max_depth = best_params$max_depth,
  eta = best_params$eta,
  gamma = best_params$gamma,
  subsample = best_params$subsample,
  colsample_bytree = best_params$colsample_bytree
)

final_xgb_model <- xgb.train(
  params = final_xgb_params,
  data = train_xgb,
  nrounds = 100,
  watchlist = list(train = train_xgb, validation = val_xgb),
  early_stopping_rounds = 10,
  verbose = 1
)

# ------------------------------------------------------------------------
# VALIDATION AND PERFORMANCE METRICS
# ------------------------------------------------------------------------

val_predictions <- predict(final_xgb_model, val_xgb)
val_class <- ifelse(val_predictions > 0.5, 1, 0)

# Ensure validation data target is a factor
val_class <- factor(val_class, levels = levels(validation_data$SURVIVAL_FLAG))

# Evaluate performance on validation set
print_metrics(final_xgb_model, validation_data, val_predictions, val_class)
```

# 2.4. Evaluation on Test data
```{r Evaluation on Test data}
# Make predictions on the test set
test_predictions <- predict(best_model, test_data, type = "class")
test_probabilities <- predict(best_model, test_data, type = "prob")[, 2]

# Call the function with predictions and probabilities
print_metrics(best_model, test_data, test_probabilities, test_predictions)

```